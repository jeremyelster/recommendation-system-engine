{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System Notebook Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. [Params](#Params)\n",
    "1. [Acquisitor and Cleaner](#Acquisitor-and-Cleaner)\n",
    "2. [Training Preparator](#Training-Preparator)\n",
    "3. [Trainer](#Trainer)\n",
    "4. [Metrics Evaluator](#Metrics-Evaluator)\n",
    "5. [Prediction Preparator](#Prediction-Preparator)\n",
    "6. [Predictor](#Predictor)\n",
    "7. [Feedback](#Feedback)\n",
    "8. [Sample Application](#Sample-Application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import marvin_recommendation_system_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"param_grid\": {\n",
    "        \"n_epochs\": [20], \n",
    "        \"lr_all\": [0.005],\n",
    "        \"reg_all\": [0.6]  \n",
    "    },\n",
    "    \"algo\": [\n",
    "        {\n",
    "            \"name\": \"KNNBaseline\",\n",
    "            \"param_grid\": {\n",
    "                'k': [40],\n",
    "                'sim_options': {\n",
    "                    'name': ['msd', 'cosine'],\n",
    "                    'min_support': [5],\n",
    "                    'user_based': [False]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"SVD\",\n",
    "            \"param_grid\": {\n",
    "                \"n_epochs\": [10], \n",
    "                \"lr_all\": [0.005],\n",
    "                \"reg_all\": [0.6]   \n",
    "            }\n",
    "        }\n",
    "            \n",
    "            \n",
    "    ],\n",
    "    \"measures\": [\"rmse\", \"mae\"],\n",
    "    \"n_cv\": 3,\n",
    "    \n",
    "    \"prediction\": {\n",
    "        \"pred_type\": \"all\",\n",
    "        \"n_pred\": 10\n",
    "    }\n",
    "}\n",
    "#epochs': [5, 10, 20], \n",
    "#'lr_all': [0.002, 0.005],\n",
    "#'reg_all': [0.4, 0.6]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aquisitor and Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "marvin_cell": "acquisitor"
   },
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "\n",
    "# Load the movielens-100k dataset (download it if needed),\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "marvin_initial_dataset = {\n",
    "    \"data\": data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Preparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "marvin_cell": "tpreparator"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.52986\n",
      "(u'196', u'302', 3.5298600000000002)\n"
     ]
    }
   ],
   "source": [
    "trainset = marvin_initial_dataset[\"data\"].build_full_trainset()\n",
    "print(trainset.global_mean)\n",
    "testset = trainset.build_anti_testset()\n",
    "print(testset[0])\n",
    "marvin_dataset = {\n",
    "    \"data\": marvin_initial_dataset[\"data\"],\n",
    "    \"trainset\": trainset,\n",
    "    \"testset\": testset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "marvin_cell": "trainer"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'param_grid': {'sim_options': {'user_based': [False], 'name': ['msd', 'cosine'], 'min_support': [5]}, 'k': [40]}, 'name': 'KNNBaseline'}\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "{'param_grid': {'lr_all': [0.005], 'reg_all': [0.6], 'n_epochs': [10]}, 'name': 'SVD'}\n"
     ]
    }
   ],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import SVD\n",
    "from surprise import KNNBaseline\n",
    "\n",
    "algo_dict = {\"SVD\": SVD, \"KNNBaseline\": KNNBaseline}\n",
    "\n",
    "model_dict = {}\n",
    "\n",
    "for algo in params[\"algo\"]:\n",
    "    \n",
    "    print(algo)\n",
    "    \n",
    "    # Get Name and Initiate Algorithm\n",
    "    algo_name = algo[\"name\"]\n",
    "    model_dict[algo_name] = {}\n",
    "        \n",
    "    # Initialize Gridsearch\n",
    "    gs = GridSearchCV(\n",
    "        algo_dict[algo_name],\n",
    "        algo[\"param_grid\"],\n",
    "        measures=params[\"measures\"],\n",
    "        cv=params[\"n_cv\"])\n",
    "    \n",
    "    gs.fit(marvin_dataset[\"data\"])\n",
    "    \n",
    "    # We can now use the algorithm that yields the best rmse:\n",
    "    best_algo = gs.best_estimator['rmse']\n",
    "    best_algo.fit(marvin_dataset[\"trainset\"])\n",
    "\n",
    "    # Get the predictions for null values in the set\n",
    "    model_dict[algo_name][\"grid_search\"] = gs\n",
    "    model_dict[algo_name][\"model\"] = best_algo\n",
    "    \n",
    "marvin_model = model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "marvin_cell": "evaluator"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: [\"sim_options: {'min_support': 5, 'name': 'msd', 'user_based': False}\", 'k: 40']\n",
      "Best RMSE: 0.939209420679\n",
      "RMSE: 0.6027\n",
      "Test Set Score: 0.602708383071\n",
      "Best Model: ['lr_all: 0.005', 'reg_all: 0.6', 'n_epochs: 10']\n",
      "Best RMSE: 0.973653369913\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import accuracy\n",
    "\n",
    "for algo in params[\"algo\"]:\n",
    "    print(algo)\n",
    "    # combination of parameters that gave the best RMSE score\n",
    "    print(\"Best Model: {}\".format([key + \": \" + str(value) for (key,value) in marvin_model[algo[\"name\"]][\"grid_search\"].best_params['rmse'].items()]))\n",
    "\n",
    "    # best RMSE score\n",
    "    print(\"Best RMSE: {}\".format(marvin_model[algo[\"name\"]][\"grid_search\"].best_score['rmse']))\n",
    "    \n",
    "    # Prediction Score\n",
    "    # Train the algorithm on the trainset, and predict ratings for the testset\n",
    "    predictions = marvin_model[algo[\"name\"]][\"model\"].test(marvin_dataset[\"testset\"])\n",
    "\n",
    "    # Then compute RMSE\n",
    "    print(\"Test Set Score: {}\".format(accuracy.rmse(predictions)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "#predictions = marvin_model[\"model\"].test(marvin_dataset[\"testset\"])\n",
    "#precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)\n",
    "\n",
    "# Precision and recall can then be averaged over all users\n",
    "#print(sum(prec for prec in precisions.values()) / float(len(precisions)))\n",
    "#print(sum(rec for rec in recalls.values()) / float(len(recalls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Preparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = {\n",
    "    \"User_id\": 196,\n",
    "    \"Item_id\": 302\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "marvin_cell": "predictor"
   },
   "outputs": [],
   "source": [
    "# get a prediction for specific users and items.\n",
    "pred_dict = {}\n",
    "\n",
    "for algo in params[\"algo\"]:\n",
    "    pred_dict[algo[\"name\"]] = marvin_model[algo[\"name\"]][\"model\"].predict(\n",
    "        str(input_message[\"User_id\"]), str(input_message[\"Item_id\"]), r_ui=4, verbose=True)\n",
    "                                           \n",
    "\n",
    "final_prediction = pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
